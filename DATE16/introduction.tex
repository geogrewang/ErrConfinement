\section{Introduction} \label{sec:introduction}
The aggressive shrinking of transistors have made circuits and especially memory cells more prone to parametric variations 
and soft errors that are expected to double for every technology generation \cite{borkar2010exascale}, thus threatening their correct functionality. The increasing demand for larger on-chip memory capacity, predicted to exceed 70\% of the die area in multiprocessors by 2017 is expected to further worsen the failure rates \cite{semiconductor2004soft}, thus indicating the need for immediate adoption of effective fault tolerant techniques.  

Techniques such as Error Correcting Codes (ECC) \cite{EmreC13} and Checkpointing \cite{dong2009leveraging} may have helped in correcting memory failures, however they incur large area, performance and power overheads ending up wasting resources and contracting with the high memory density requirements. In an effort to limit such overheads recent approaches exploit the tolerance to faults/approximations of many applications  \cite{chippa2013analysis} and relax the requirement of 100\% correctness. The main idea of such methods
is the restricted use of robust but power hungry bit-cells and methods such as ECC to protect only the bits that play a more significant role in shaping the output quality \cite{YuehCM13} \cite{LeeKPP13}. Few very recent approaches exist also that extend generic instruction sets with approximation features and specialized hardware units \cite{esmaeilzadeh2012architecture}\cite{venkataramani2013quality}\cite{sampson2011enerj}. Although such techniques are very interesting and showcased the available possibilities in certain applications they are still based on redundancy and have neglected to exploit some more fundamental characteristics of the application data.    
    
\textbf{\textit{Contribution}} In this paper, we enhance the state of the art by proposing an alternative system level method for mitigating memory failures and presenting the necessary software and hardware features for realizing it within a RISC processor. 
The proposed approach, instead of adding circuit level redundancy to correct memory errors tries to limit the impact of those errors in the output quality by replacing any erroneous data with the best available estimate of those data. The proposed approach is realized by enhancing a common programming model and a RISC processor with custom instructions and low cost hardware support modules. We demonstrate the low overhead and error mitigation ability of  the proposed approach by applying it on the different algorithmic stages of JPEG and comparing with the extensively used Single Error Correction Double Error Detection (SECDED) method. Overall, the proposed scheme offers better error confinement since it is based on application specific statistical characteristics, while allowing to mitigate single and multiple bit errors with substantially less overheads.  

The rest of work is organized as following. Section \ref{sec:theory} introduces the proposed approach while Section \ref{sec:qos_asip} describes the enhancements of a processor for realizing it. Section \ref{sec:app} presents the statistical analysis of the proposed approach. Section \ref{sec:exp} presents the simulation results. Finally, Section \ref{sec:conclusion} concludes the work.
  
%George: ECC explanation can go in a later section 
%To counter the memory failures, Error Correcting Code (ECC) and Checkpointing techniques are widely used. The most common %standard of ECC is the Single Error Correction Double Error Detection (SECDED) which is able to detect double bit errors and %correct single bit error on the fly. Hamming code and its derivatives are often applied as an low-cost implementation of the ECC %due to its fast online correction capabilities. When more than one bit is detected, the system rolls back to its previous state %of snapshot which is the checkpointing technique. Consequently, a stronger ECC will reduce the frequency of checkpointing so |%that less snapshots are taken. Other ECC such as BCH codes are also proposed in \cite{li2011system}. Detailed investigation on %the effects of checkpointing can be referred in \cite{dong2009leveraging}.

%While effective in increase of system reliability, both ECC and Checkpointing incur significantly design overhead such as area, %access latency and power consumption. For instance, hamming ECC uses 6 bits code for protection of 32 bits data word while 8 %bits for 64 bits data. For every load and store operation, the ECC incurs additional power and delay in encoding, decoding and %correction through specific logic. Advanced ECC such as BCH codes consumes more delay in error correction to reduce the %processing throughput. For detailed analysis on system-level impact of ECC readers can refer to \cite{li2011system}. Such %overheads are to some extent tolerable in mainstream and supercomputing domain, however, meet difficulty in its usage for %embedded and real time applications where energy and timing constraints are strong requirements. To trade off reliability with %performance loss and power consumption in embedded domain, recently lots of research bodies have been investigated on the area %of approximate computing, where application-specific quality of service degradation is tolerable. 

%Approximate computing techniques have been proposed at various design abstraction layers from circuits \cite{kahng2012accuracy}%%\cite{gupta2011impact}\cite{ernst2003razor}, architecture \cite{lingamneni2012algorithmic}\cite{chippa2011dynamic} to algorithm %\cite{chippa2010scalable}\cite{chippa2013analysis} and system-level \cite{mitra_ersa}, which are usually implemented by static %or dynamic quality configuration according to the requirements of computational accuracy. On the other hand, a few tools which %supporting synthesizing inexact circuits by boolean and algebraic optimization techniques were proposed in %%%\cite{venkatesan2011macaco}\cite{venkataramani2012salsa}\cite{lingamneni2013synthesizing}. There also exists approaches to extend %generic programming language with data approximation features using custom instruction sets \cite{esmaeilzadeh2012architecture}%\cite{venkataramani2013quality}\cite{sampson2011enerj}, where additional hardware supports are needed. \cite{roth2012data} and \cite{chippa2011dynamic} also highlight the effects of unreliable memories and introduce approximate techniques to improve the %error rate. Generally, state-of-the-art techniques in approximate computing are applied during the design phase of algorithm and %architecture, while we investigate in parallel on its possibility as a technique for correction of errors in memory.

%\textbf{\textit{Contribution}} In this paper we propose a low cost error correction technique for memories based on \textsl{data %approximation} (DA), which exhibits frequently in data and image processing applications. It takes advantage of the similarity %of data pattern during data processing, and implies it as a pre-knowledge for online data correction. In contrast to the large %overhead of ECC, single bit parity is used for error detection. We show the usability of the proposed technique in JPEG %application as an example and shows the proposed technique achieves similar peak Signal-to-noise Ratio (PSNR) with significantly %less memory usage. Furthermore, we extend an embedded processor with additional hardware modules supporting custom instructions %and introduce new programming models using the concept of data approximation.

%The rest of work is organized as following. Section \ref{sec:theory} introduces the principle of DA. Section \ref{sec:qos_asip} %describes the custom processor architecture and its programming example with instruction supporting DA. Section \ref{sec:jpeg} %analyzes JPEG application as an example which shows the feature of DA. Section \ref{sec:exp} presents detailed experiments for %error correction using DA approach for JPEG. Section \ref{sec:conclusion} concludes the work and directs the further research.
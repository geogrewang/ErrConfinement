\section{Introduction} \label{sec:introduction}
As technology continuously scales, memory reliability is the key issue in modern system design. According to \cite{Shalf2010ECT}, main memory is one of the most vulnerable hardware components which contributes to 40\% of the hardware related failures. Such trend will further increase due to the large capacity of memory integration and the new technology adopted with smaller driving voltage. In \cite{borkar2010exascale} it is predicted that the soft error rate doubles for every generation of technology due to the reduced signal-to-noise ratio (SNR). Besides, the increased memory density will suffer the chip from more bits error when facing the alpha particles. As a reference, \cite{semiconductor2004soft} documents the failure rate for both soft and hard errors for 130nm and 90nm technologies.

To counter the memory failures, Error Correcting Code (ECC) and Checkpointing techniques are widely used. The most common standard of ECC is the Single Error Correction Double Error Detection (SECDED) which is able to detect double bit errors and correct single bit error on the fly. Hamming code and its derivatives are often applied as an low-cost implementation of the ECC due to its fast online correction capabilities. When more than one bit is detected, the system rolls back to its previous state of snapshot which is the checkpointing technique. Consequently, a stronger ECC will reduce the frequency of checkpointing so that less snapshots are taken. Other ECC such as BCH codes are also proposed in \cite{li2011system}. Detailed investigation on the effects of checkpointing can be referred in \cite{dong2009leveraging}.

While effective in increase of system reliability, both ECC and Checkpointing incur significantly design overhead such as area, access latency and power consumption. For instance, hamming ECC uses 6 bits code for protection of 32 bits data word while 8 bits for 64 bits data. For every load and store operation, the ECC incurs additional power and delay in encoding, decoding and correction through specific logic. Advanced ECC such as BCH codes consumes more delay in error correction to reduce the processing throughput. For detailed analysis on system-level impact of ECC readers can refer to \cite{li2011system}. Such overheads are to some extent tolerable in mainstream and supercomputing domain, however, meet difficulty in its usage for embedded and real time applications where energy and timing constraints are strong requirements. To trade off reliability with performance loss and power consumption in embedded domain, recently lots of research bodies have been investigated on the area of approximate computing, where application-specific quality of service degradation is tolerable. 

Approximate computing techniques have been proposed at various design abstraction layers from circuits \cite{kahng2012accuracy}\cite{gupta2011impact}\cite{ernst2003razor}, architecture \cite{lingamneni2012algorithmic}\cite{chippa2011dynamic} to algorithm \cite{chippa2010scalable}\cite{chippa2013analysis} and system-level \cite{mitra_ersa}, which are usually implemented by static or dynamic quality configuration according to the requirements of computational accuracy. On the other hand, a few tools which supporting synthesizing inexact circuits by boolean and algebraic optimization techniques were proposed in \cite{venkatesan2011macaco}\cite{venkataramani2012salsa}\cite{lingamneni2013synthesizing}. There also exists approaches to extend generic programming language with data approximation features using custom instruction sets \cite{esmaeilzadeh2012architecture}\cite{venkataramani2013quality}\cite{sampson2011enerj}, where additional hardware supports are needed. \cite{roth2012data} and \cite{chippa2011dynamic} also highlight the effects of unreliable memories and introduce approximate techniques to improve the error rate. Generally, state-of-the-art techniques in approximate computing are applied during the design phase of algorithm and architecture, while we investigate in parallel on its possibility as a technique for correction of errors in memory.

\textbf{\textit{Contribution}} In this paper we propose a low cost error correction technique for memories based on data approximation, which exhibits frequently in data and image processing applications. It takes advantage of the similarity of data pattern during data processing, and implies it as a pre-knowledge for online data correction. In contrast to the large overhead of ECC, single bit parity is used for error detection. We show the usability of the proposed technique in JPEG application as an example and shows the proposed technique achieves similar peak Signal-to-noise Ratio (PSNR) with significantly less memory usage. Furthermore, we extend an embedded processor with additional hardware modules supporting custom instructions and introduce new programming models using the concept of data approximation.

The rest of work is organized as following. Section \label{sec:data_approx} introduces data approximation and its properties in JPEG application. Section \ref{sec:exp} conducts experiments in error correction with data approximation and benchmark its efficiency and overhead with ECC approach. Section \ref{sec:qos_asip} presents the processor architecture with custom instructions and its programming model using approximate correction. Section \ref{sec:conclusion} concludes the work as directs the further research.